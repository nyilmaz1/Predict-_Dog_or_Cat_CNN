{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#1.1: IMPORTING THE KERAS LIBRARIES AND PACKAGES\n",
    "\n",
    "from keras.models import Sequential #To initialize our NN\n",
    "from keras.layers import Conv2D #Step 1: adding convolutional layers. 2D for images\n",
    "from keras.layers import MaxPooling2D #step 2: pooling\n",
    "from keras.layers import Flatten #step 3: flattening\n",
    "from keras.layers import Dense #step 4: add fully connected layers in classic ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2: INITIALIZE THE CNN\n",
    "\n",
    "classifier = Sequential() #we create the classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "#1.3: STEP 1: ADD CONVOLUTION LAYER:\n",
    "\n",
    "classifier.add(Conv2D(32, (3,3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "#Conv2D arguments:\n",
    "#first argument is the number of filters/feature detectors = 32, it's also the number of feature maps\n",
    "#for our first convolutional layer, we will use 32 filters, but we can increase it with the additional layers\n",
    "#(3,3) number of rows and columns for the feature detectors\n",
    "#input_shape: shape of your input image on which we will apply the feature detector\n",
    "#colored images are 3D arrays; black and white are 2D arrays\n",
    "#we're working with colored images\n",
    "#3 is the number of channels; if we are dealing with black and white this would be 1\n",
    "#64, 64 are the dimensions of the 2d array of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4: STEP 2: ADD POOLING:\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#maxpooling2d arguments:\n",
    "#pool_size = is the size of the pooled feature map 2by 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.5: ADD SECOND CONVOLUTIONAL LAYER:\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6: STEP 3: FLATTENING:\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.7: STEP 4: FULL CONNECTION: CLASSIC ANN\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "#units: the number of output nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.8: COMPILING THE CNN: to optimize\n",
    "classifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2) Fitting the CNN to the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 84s 334ms/step - loss: 0.6726 - acc: 0.5939 - val_loss: 0.6375 - val_acc: 0.6395\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 81s 326ms/step - loss: 0.6395 - acc: 0.6315 - val_loss: 0.6006 - val_acc: 0.6850\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 82s 329ms/step - loss: 0.5977 - acc: 0.6785 - val_loss: 0.5884 - val_acc: 0.6965\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 76s 305ms/step - loss: 0.5644 - acc: 0.7079 - val_loss: 0.5827 - val_acc: 0.7025\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.5340 - acc: 0.7332 - val_loss: 0.5298 - val_acc: 0.7460\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.5087 - acc: 0.7515 - val_loss: 0.5318 - val_acc: 0.7465\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 84s 336ms/step - loss: 0.4870 - acc: 0.7640 - val_loss: 0.5410 - val_acc: 0.7425\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 72s 290ms/step - loss: 0.4714 - acc: 0.7765 - val_loss: 0.4734 - val_acc: 0.7850\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 71s 284ms/step - loss: 0.4432 - acc: 0.7921 - val_loss: 0.4824 - val_acc: 0.7770\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 76s 303ms/step - loss: 0.4368 - acc: 0.7954 - val_loss: 0.4888 - val_acc: 0.7675\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 73s 293ms/step - loss: 0.4172 - acc: 0.8081 - val_loss: 0.5060 - val_acc: 0.7610\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 74s 297ms/step - loss: 0.3954 - acc: 0.8201 - val_loss: 0.4721 - val_acc: 0.7865\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 72s 289ms/step - loss: 0.3826 - acc: 0.8281 - val_loss: 0.4660 - val_acc: 0.7965\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 92s 368ms/step - loss: 0.3720 - acc: 0.8336 - val_loss: 0.4595 - val_acc: 0.7965\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 0.3493 - acc: 0.8464 - val_loss: 0.4653 - val_acc: 0.7970\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 79s 318ms/step - loss: 0.3371 - acc: 0.8549 - val_loss: 0.5131 - val_acc: 0.7860\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 0.3246 - acc: 0.8561 - val_loss: 0.4640 - val_acc: 0.8000\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 79s 317ms/step - loss: 0.3056 - acc: 0.8656 - val_loss: 0.4707 - val_acc: 0.8040\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 0.3009 - acc: 0.8710 - val_loss: 0.4733 - val_acc: 0.8105\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.2751 - acc: 0.8806 - val_loss: 0.4850 - val_acc: 0.8130\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 79s 317ms/step - loss: 0.2617 - acc: 0.8881 - val_loss: 0.4757 - val_acc: 0.8105\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 78s 314ms/step - loss: 0.2492 - acc: 0.8962 - val_loss: 0.4728 - val_acc: 0.8105\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.2306 - acc: 0.9052 - val_loss: 0.5396 - val_acc: 0.7965\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.2244 - acc: 0.9095 - val_loss: 0.5574 - val_acc: 0.7870\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.2158 - acc: 0.9137 - val_loss: 0.5111 - val_acc: 0.8145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18195d21d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#image augmentation: to prevent overfitting\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "#datagen part is the image augmentation part\n",
    "#rescale: corresponds to the feauture scaling of data preprocessing\n",
    "#shear_range: the pixels are moved to a fixed direction \n",
    "#zoom_range: random zoom that we apply on our images\n",
    "#horizontal_flip: flips the images\n",
    "#we also have vertical flip but that is not used here\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/Users/nevinyilmaz/Desktop/UDEMY DEEP LEARNING A-Z/PART 2-Convolutional_Neural_Networks/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/Users/nevinyilmaz/Desktop/UDEMY DEEP LEARNING A-Z/PART 2-Convolutional_Neural_Networks/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000/32,#number of images in the training set\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000/32)#32 is our batch size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3) Making new predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/Users/nevinyilmaz/Desktop/UDEMY DEEP LEARNING A-Z/PART 2-Convolutional_Neural_Networks/dataset/single_prediction/cat_or_dog_5.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "test_image = test_image/(255.0)\n",
    "result = classifier.predict_classes(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see 0 and 1 correspondents\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 310,
   "position": {
    "height": "332px",
    "left": "395px",
    "right": "20px",
    "top": "156px",
    "width": "551px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
